{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from IPython.display import display\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splited train_labels.csv to train and validation csv file \n",
    "download the dataset from https://www.kaggle.com/c/cs-t0828-2020-hw1/data\n",
    "put the \"training_data\" file and \"testing file\" all in this repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data():\n",
    "#     data = pd.read_csv(\"train_label.csv\")\n",
    "#     data = data.iloc[:300]\n",
    "#     labels = data.groupby(\"label\")\n",
    "#     np.random.seed(10)\n",
    "#     for i, label in enumerate(data.label.unique()):\n",
    "#         randomList = list(range(len(labels.get_group(label))))\n",
    "#         np.random.shuffle(randomList)\n",
    "#         trainPart = labels.get_group(label).iloc[randomList][:int(len(randomList)*0.8)]\n",
    "#         testPart = labels.get_group(label).iloc[randomList][int(len(randomList)*0.8):]\n",
    "#         if (i == 0):\n",
    "#             trainAll = trainPart\n",
    "#             testAll = testPart\n",
    "#         else:\n",
    "#             trainAll = pd.concat([trainAll, trainPart], axis=0)\n",
    "#             testAll = pd.concat([testAll, testPart], axis=0)\n",
    "#     trainAll.to_csv(\"train.csv\",index=False)\n",
    "#     testAll.to_csv(\"valid.csv\",index=False)\n",
    "# split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mount image to image files according to there label_index if needed \n",
    "do it twice for train.csv and valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootPath = \"training_data/training_data//\"\n",
    "# df = pd.read_csv('valid.csv')\n",
    "# for i, x in df.iterrows():\n",
    "#     print(x.id, x.label)\n",
    "#     img = Image.open(os.path.join(rootPath, str(x.id).zfill(6)+'.jpg'))\n",
    "#     if not os.path.isdir(f\"./car_dataset/test/{x.label_index}\"):\n",
    "#         os.mkdir(f\"./car_dataset/test/{x.label_index}\")\n",
    "#     img.save(f\"./car_dataset/test/{x.label_index}/{str(x.id).zfill(6)}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation, data normalization, and load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./car_dataset/\"\n",
    "batch_size = 16\n",
    "train_tfms = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomRotation(15),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_tfms = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=dataset_dir+\"train\", transform=train_tfms)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset2 = torchvision.datasets.ImageFolder(root=dataset_dir+\"test\", transform=test_tfms)\n",
    "testloader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, n_epochs=5):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_accuracies = []\n",
    "    # set the model to train mode initially\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs and assign them to cuda\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # calculate the loss/acc later\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels == predicted).sum().item()\n",
    "        epoch_duration = time.time()-since\n",
    "        epoch_loss = running_loss/len(trainloader)\n",
    "        epoch_acc = 100/batch_size*running_correct/len(trainloader)\n",
    "        print(\"Epoch %s, duration: %d s, loss: %.4f, acc: %.4f\" % (epoch+1, epoch_duration, epoch_loss, epoch_acc))\n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_acc)\n",
    "        # switch the model to eval mode to evaluate on test data\n",
    "        model.eval()\n",
    "        test_acc = eval_model(model)\n",
    "        test_accuracies.append(test_acc)\n",
    "        # re-set the model to train mode after validating\n",
    "        model.train()\n",
    "        scheduler.step(test_acc)\n",
    "        since = time.time()\n",
    "    print('Finished Training')\n",
    "    return model, losses, accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model_ft(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_acc = 100.0 * correct / total\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        test_acc))\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain model\n",
    "model_ft = models.resnet152(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# replace the last fc layer with an untrained one (requires grad by default)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 196)\n",
    "model_ft = model_ft.to(device)\n",
    "# loss function and optimzer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft, training_losses, training_accs, test_accs = train_model(model_ft, criterion, optimizer, lrscheduler, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save({\"epoch\": 10, \n",
    "           \"model_state_dict\": model_ft.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict()},\n",
    "           \"model/resnet152.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load parameter without retraining again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model trained parameters\n",
    "checkpoint = torch.load(\"model/resnet152.pth\")\n",
    "model_ft.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classes.txt find accrodingly class name\n",
    "classes = pd.read_table(\"classes.txt\", names=[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(dir):\n",
    "    classes = os.listdir(dir)\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "pp, c_to_idx = find_classes(dataset_dir+\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch the model to evaluation mode to make dropout and batch norm work in eval mode\n",
    "model_ft.eval()\n",
    "\n",
    "# transforms for the input image\n",
    "loader = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "results = pd.DataFrame(data={'id': [], 'label': []})\n",
    "# load test image from file\n",
    "for file in os.listdir(\"testing_data/testing_data/\"):\n",
    "    try:\n",
    "\n",
    "        image = Image.open(os.path.join('testing_data/testing_data', file))\n",
    "        image = image.convert('RGB')\n",
    "        image = loader(image).float()\n",
    "    except RuntimeError:\n",
    "\n",
    "        print(file)\n",
    "        break\n",
    "    image = torch.autograd.Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    output = model_ft(image)\n",
    "    conf, predicted = torch.max(output.data, 1)\n",
    "    label = classes.loc[int(pp[predicted.item()])]['class']\n",
    "    idx = os.path.splitext(file)[0]\n",
    "    # print image id, label_index of image file, accordingly class name, predicted confidence(probability)\n",
    "    print(idx, label, predicted.item(), conf)\n",
    "    results = results.append({'id': idx, 'label': label}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "results.to_csv('results.csv', index=False, header=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
